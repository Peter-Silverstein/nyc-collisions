---
title: "Causal Analysis of the Effect of Congestion Pricing on Traffic Accidents in NYC"
author: "Peter Silverstein"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
# Libraries
library(tidyverse)
library(sf)
library(tidycensus)
library(keyring)
library(DT)
library(tmap)
library(spdep)
library(cmdstanr)
library(posterior)
library(bayesplot)
library(loo)
library(parallel)
cores = floor(parallel::detectCores() / 2)
source("helper-functions/utils_dataviz.R")
source("helper-functions/utils_bym2.R")
```

# Introduction

This is part III of my personal project trying to use a variety of Stan modeling techniques to model collisions in NYC. This project is largely based on the NYC Collisions Dataset (INSERT LINK). In part I, I performed exploratory data analysis to better understand the data, as well as beginning to examine the sub-theme pattern in these data: whether the implementation of congestion pricing had an effect on crash numbers in the congestion pricing zone. In part II, I followed an excellent guide from Mitzi Morris to build a model of variation due to spatial dependency. This model effectively captured whether variation in the response variable (crash count) was due to spatial correlation or other random effects. The downside of this model was that it didn't have any real predictive ability--it couldn't handle new data well. That's sort of expected, given that each census tract only had one observation.

Here in part III, my goal is to extend the model such that I can include a treatment effect. To do this, I need to add an additional source of variation to my model: time. That means I would have (a) variation due to the month and year in which the observation occurred, (b) variation correlated with neighboring census tracts, and (c) other within-tract heterogeneity. I will also include some covariate predictors in my model, as well as the treatment indicator.

I will begin with a very simple Poisson model accounting for month-level and year-level effects using a super traditional hierarchical structure. 

# Data Preparation

The data preparation for this part is largely the same as the previous part, with two big exceptions. First, data are aggregated to the Tract/Month/Year level, to provide the hierarchical structure that my model needs. Second, I have included an indicator variable for whether the observed tract is within the congestion pricing zone and, for those that are, an indicator for whether the observed point is 01/2025 or later. I consider a tract observation within the zone from 01/2025 or later to be "treated."

```{r class.source=NULL, message=FALSE, warning=FALSE}
# Loading NYC Census Tracts: https://data.cityofnewyork.us/City-Government/2020-Census-Tracts/63ge-mke6/about_data 
nyc_tracts <- read.csv("data/2020_Census_Tracts_20250606.csv", stringsAsFactors = FALSE) %>%
  rename("geometry" = "the_geom")
nyc_tracts <- st_as_sf(nyc_tracts, wkt = "geometry", crs = 4326)
nyc_tracts <- nyc_tracts %>%
  mutate(
    area_sqm = Shape_Area / 27878400
  ) %>%
  select(
    geometry,
    BoroCT2020,
    BoroCode,
    BoroName,
    area_sqm,
    GEOID
  )

# Loading collisions data: https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data 
filter <- c(0, NA)

crashes <- read.csv("data/Motor_Vehicle_Collisions_-_Crashes_20250605.csv") %>%
  select(CRASH.DATE,
         LATITUDE,
         LONGITUDE,
         NUMBER.OF.PERSONS.INJURED,
         NUMBER.OF.PERSONS.KILLED,
         NUMBER.OF.PEDESTRIANS.INJURED,
         NUMBER.OF.PEDESTRIANS.KILLED) %>%
  filter(! LATITUDE %in% filter,
         ! LONGITUDE %in% filter) %>%
  rename(
    "date" = "CRASH.DATE",
    "lat" = "LATITUDE",
    "long" = "LONGITUDE",
    "persons_inj" = "NUMBER.OF.PERSONS.INJURED",
    "persons_death" = "NUMBER.OF.PERSONS.KILLED",
    "ped_inj" = "NUMBER.OF.PEDESTRIANS.INJURED",
    "ped_death" = "NUMBER.OF.PEDESTRIANS.KILLED"
  ) %>%
  mutate(
    date = mdy(date)
    ) %>%
  st_as_sf(coords = c("long","lat"), crs = 4326)

# Grabbing some census variables via Tidycensus, using Keyring for API Key privacy
tidycensus_api_key <- key_get(service = "tidycensus_API", username = "my_tidycensus")
census_api_key(tidycensus_api_key)

census_vars <- get_acs(state = "NY",
                       county = c("Bronx", "Kings", "New York", "Queens", "Richmond"),
                       geography = "tract",
                       variables = c(medincome = "B19013_001",
                                     population = "B01003_001",
                                     median_age = "B01002_001",
                                     transport_baseline = "B08301_001",
                                     transport_pubtransit = "B08301_010"),
                       geometry = FALSE,
                       keep_geo_vars = FALSE,
                       year = 2022,
                       output = "wide"
                       ) %>%
  mutate(
    GEOID = as.numeric(GEOID),
    median_income = medincomeE,
    population = populationE,
    median_age = median_ageE,
    prop_pubtransit = transport_pubtransitE / transport_baselineE
  ) %>%
  select(
    GEOID,
    median_income,
    population,
    median_age,
    prop_pubtransit
  )

# Associating Borough, and Census Tract w/ Observations
crashes <- crashes %>%
  st_join(nyc_tracts, join = st_within) %>%
  filter(! is.na(area_sqm))

# Loading Central Business District Shape: https://data.ny.gov/Transportation/MTA-Central-Business-District-Geofence-Beginning-J/srxy-5nxn/about_data
cbd_geofence <- read.csv("data/MTA_Central_Business_District_Geofence__Beginning_June_2024_20250605.csv", stringsAsFactors = FALSE)
cbd_geofence <- st_as_sfc(cbd_geofence$polygon, crs = 4326)

# Aggregating/summarizing data to census tract, month, year levels
# Note the included components to fill month/year/tract combination with no crashes with 0
crashes_tract <- crashes %>%
  mutate(
    monthyear = format(date, "%m-%y")
  ) %>%
  group_by(BoroCT2020, monthyear) %>%
  summarize(tot_crashes = n(),
            area = mean(area_sqm),
            .groups = "drop") %>%
  complete(BoroCT2020, monthyear, fill = list(tot_crashes = 0)) %>% 
  group_by(BoroCT2020) %>%
  fill(area, .direction = "downup") %>%
  ungroup() %>%
  st_drop_geometry() %>%
  select(! geometry) %>%
  mutate(
    month = as.numeric(substr(monthyear, 1, 2)),
    year = as.numeric(substr(monthyear, 4, 5))
  ) %>%
  filter(! (year == 25 & month > 5))

# Getting Fragmentation Index from: https://github.com/mitzimorris/geomed_2024/blob/main/data/nyc_study.geojson

frag_data = st_read(file.path("data", "nyc_study.geojson"), quiet = TRUE) %>%
  st_drop_geometry() %>%
  select(BoroCT2010, frag_index, traffic) %>%
  mutate(
    BoroCT2010 = as.numeric(BoroCT2010)
  )

# Joining everything together, selecting only variables that I want
crashes_tracts_geo <- nyc_tracts %>%
  right_join(crashes_tract, 
             by = "BoroCT2020") %>%
  left_join(census_vars,
            by = "GEOID") %>%
  left_join(frag_data,
            by = c("BoroCT2020" = "BoroCT2010")) %>%
  select(! c(area)) %>% 
  filter(! if_any(everything(), is.na)) %>%
  mutate(
    cp_zone = as.integer(lengths(st_intersects(geometry, cbd_geofence)) > 0),
    after_cp = ifelse(year == 25, 1, 0),
    treatment = ifelse(cp_zone == 1 & after_cp == 1, 1, 0)
  ) %>%
  mutate(
    tract_id = as.integer(factor(BoroCT2020)),
    time_id = as.integer(factor(monthyear))
  )

# Interactive Data Table for Display
crashes_dt <- crashes_tracts_geo %>%
  st_drop_geometry() %>%
  mutate(
    area_sqm = round(area_sqm, 3),
    prop_pubtransit = round(prop_pubtransit, 3)
  )

datatable(crashes_dt,
          extensions = 'Buttons',
          filter = "top",
          rownames = FALSE,
          options = list(
            autoWidth = TRUE,
            scrollX = TRUE
          ),
  class = 'compact',
  escape = FALSE
) %>%
  formatStyle(
    columns = names(crashes_dt),
    `white-space` = "nowrap",
    `height` = "1.5em",
    `line-height` = "1.5em"
  )
```

## Visualizing the Distribution of Counts

```{r}
ggplot(data = crashes_tracts_geo, aes(x = tot_crashes)) + 
  geom_histogram(aes(y = after_stat(density)), 
                 bins = 100, 
                 color = "lightblue",
                 fill = "lightblue",
                 alpha = 0.75) + 
  theme_bw() + 
  labs(title = "Distribution Total Crashes per Census Tract (2024-25)",
       x = "Number of Crashes",
       y = "Density")
```

# Time Series Modeling

## Initial Time Series Poisson Model

```{r class.source=NULL, message=FALSE, warning=FALSE}
# The Stan Model
time_model_1_file = file.path('stan', 'poisson_time_1.stan')
cat(readLines(time_model_1_file), sep="\n")
```

```{r}
# Fitting the Model, Examining the Output

# Setting up input data
design_matrix <- as.data.frame(crashes_tracts_geo) %>%
  select(prop_pubtransit, median_income, median_age, frag_index, traffic) %>%
  mutate(median_income = log(median_income),
         traffic = log(traffic))
  
N <- nrow(crashes_tracts_geo)
y <- as.integer(crashes_tracts_geo$tot_crashes)
E <- as.integer(crashes_tracts_geo$population)
K <- ncol(design_matrix)
xs <- design_matrix
I <- length(unique(crashes_tracts_geo$tract_id))
i_index <- crashes_tracts_geo$tract_id
J <- length(unique(crashes_tracts_geo$time_id))
j_index <- crashes_tracts_geo$time_id

data_time_1 <- list(
  N = N,
  y = y,
  E = E,
  K = K,
  xs = xs,
  I = I,
  J = J,
  i_index = i_index,
  j_index = j_index
)

time_model_1 <- cmdstan_model(time_model_1_file)
set.seed(50)

time_fit_1 <- time_model_1$sample(data=data_time_1, 
                                  parallel_chains=cores, 
                                  refresh=0)

print(time_fit_1)
```

```{r class.source=NULL, message=FALSE, warning=FALSE}
# Posterior Checks
y_rep_time <- as_draws_matrix(time_fit_1$draws("y_rep"))
ppc_central_interval(y_rep_time, data_time_1$y)
```

```{r class.source=NULL, message=FALSE, warning=FALSE}
ppc_y_yrep_overlay(y_rep_time, data_time_1$y,
                     'Poisson model PPC\ny (blue dot) vs. y_rep (yellow 50% central interval, grey full extent)')
```

```{r}
# Check for Overdispersion
mean_y <- mean(data_time_1$y)
var_y <- var(data_time_1$y)

cat("Mean:", mean_y, "Variance:", var_y, "Ratio:", var_y / mean_y, "\n")
```

## Time Series Negative Binomial Model

```{r class.source=NULL, message=FALSE, warning=FALSE}
# The Stan Model
time_model_2_file = file.path('stan', 'poisson_time_2.stan')
cat(readLines(time_model_2_file), sep="\n")
```

```{r}
# Fitting the Model, Examining the Output

# Setting up input data
design_matrix <- as.data.frame(crashes_tracts_geo) %>%
  select(prop_pubtransit, median_income, median_age, frag_index, traffic) %>%
  mutate(median_income = log(median_income),
         traffic = log(traffic))
  
N <- nrow(crashes_tracts_geo)
y <- as.integer(crashes_tracts_geo$tot_crashes)
E <- as.integer(crashes_tracts_geo$population)
K <- ncol(design_matrix)
xs <- design_matrix
I <- length(unique(crashes_tracts_geo$tract_id))
i_index <- crashes_tracts_geo$tract_id
J <- length(unique(crashes_tracts_geo$time_id))
j_index <- crashes_tracts_geo$time_id

data_time_2 <- list(
  N = N,
  y = y,
  E = E,
  K = K,
  xs = xs,
  I = I,
  J = J,
  i_index = i_index,
  j_index = j_index
)

time_model_2 <- cmdstan_model(time_model_2_file)
set.seed(50)

time_fit_2 <- time_model_2$sample(data=data_time_2, 
                                  parallel_chains=cores, 
                                  refresh=0)

time_summary <- time_fit_2$summary()
vars <- c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'betas[5]', 'sigma', 'rho', 'nb_disp')
time_summary_subset <- time_summary[time_summary$variable %in% vars, ]
numeric_cols <- sapply(time_summary_subset, is.numeric)
time_summary_subset[numeric_cols] <- round(time_summary_subset[numeric_cols], 3)

print(time_summary_subset)
```

```{r class.source=NULL, message=FALSE, warning=FALSE}
# Posterior Checks
y_rep_time2 <- as_draws_matrix(time_fit_2$draws("y_rep"))
ppc_central_interval(y_rep_time2, data_time_2$y)
```

```{r class.source=NULL, message=FALSE, warning=FALSE}
ppc_y_yrep_overlay(y_rep_time2, data_time_2$y,
                     'Poisson model PPC\ny (blue dot) vs. y_rep (yellow 50% central interval, grey full extent)')
```

```{r}
# Check Zero Inflation
n_zeros_obs <- sum(data_time_2$y == 0)
n_zeros_pred <- apply(y_rep_time2, 1, function(x) sum(x == 0))
mean_pred_zeros <- mean(n_zeros_pred)

print(paste("Actual N Zeros:", n_zeros_obs, "Mean Pred Zeros:", mean_pred_zeros))
```

# Combining BYM2 and Time Series Models

## Retrieving the Spatial Dependency Matrix

```{r}
# Get the spatial structure
tracts_uniques <- crashes_tracts_geo %>%
  select(tract_id, geometry, BoroCT2020) %>%
  distinct() %>%
  st_set_crs(st_crs(crashes_tracts_geo))

nyc_nbs = poly2nb(tracts_uniques)
nyc_coords = st_coordinates(st_centroid(tracts_uniques['geometry']))

# Create a spatial points sf object for centroids
centroids_sf <- st_as_sf(data.frame(x = nyc_coords[,1], y = nyc_coords[,2]), 
                         coords = c("x", "y"), 
                         crs = st_crs(tracts_uniques))

# Function to add a symmetric edge
add_symmetric_edge_nb <- function(nb, i, j) {
  nb_res = nb
  if (!(j %in% nb[[i]])) {
    if (nb[[i]][1] == 0) {
      nb_res[[i]] = j
    } else {
      nb_res[[i]] <- sort(c(nb[[i]], j))
    }
  }
  if (!(i %in% nb[[j]])) {
    if (nb[[j]][1] == 0) {
      nb_res[[j]] = i
     } else {
       nb_res[[j]] <- sort(c(nb[[j]], i))
     }
  }
  return(nb_res)
}

nyc_nbs_connected = add_symmetric_edge_nb(nyc_nbs, 
                                          which(tracts_uniques$BoroCT2020 == 3016400),
                                          which(tracts_uniques$BoroCT2020 == 5001800))
nyc_nbs_connected = add_symmetric_edge_nb(nyc_nbs_connected, 
                                          which(tracts_uniques$BoroCT2020 == 4088400),
                                          which(tracts_uniques$BoroCT2020 == 4107201))
nyc_nbs_connected = add_symmetric_edge_nb(nyc_nbs_connected, 
                                          which(tracts_uniques$BoroCT2020 == 4107201),
                                          which(tracts_uniques$BoroCT2020 == 4094201))
nyc_nbs_connected = add_symmetric_edge_nb(nyc_nbs_connected, 
                                          which(tracts_uniques$BoroCT2020 == 4099801),
                                          which(tracts_uniques$BoroCT2020 == 4103202))
nyc_nbs_connected = add_symmetric_edge_nb(nyc_nbs_connected, 
                                          which(tracts_uniques$BoroCT2020 == 2045600),
                                          which(tracts_uniques$BoroCT2020 == 2042600))

nyc_nbs_connected = add_symmetric_edge_nb(nyc_nbs_connected, 
                                          which(tracts_uniques$BoroCT2020 == 1029900),
                                          which(tracts_uniques$BoroCT2020 == 2026900))

lines_sp = nb2lines(nyc_nbs_connected, coords = nyc_coords, proj4string = st_crs(tracts_uniques)$wkt)
lines_sf = st_as_sf(lines_sp)

# Checking Work
spdep_map <- tm_shape(tracts_uniques) +
  tm_polygons(col = "white") + 
  tm_lines(col = "white", lwd = 2) + 

  # Add points for centroids
  tm_shape(centroids_sf) +
  tm_dots(size = 0.25, col = "black") +

  # Add lines for neighbors
  tm_shape(lines_sf) +
  tm_lines(col = "deepskyblue3")

spdep_map
```

```{r}
cat('is symmetric? ', is.symmetric.nb(nyc_nbs_connected), '\n')
summary(nyc_nbs_connected)
cat('nodes per component')
table(n.comp.nb(nyc_nbs_connected)$comp.id)
```

## Modeling

```{r}
# The Stan Model
full_model_1_file = file.path('stan', 'full_model_1.stan')
cat(readLines(full_model_1_file), sep="\n")
```

```{r}
# Setting up input data
nbs_adj <- nbs_to_adjlist(nyc_nbs_connected)

design_matrix <- as.data.frame(crashes_tracts_geo) %>%
  select(prop_pubtransit, median_income, median_age, frag_index, traffic) %>%
  mutate(median_income = log(median_income),
         traffic = log(traffic))
  
N <- nrow(crashes_tracts_geo)
y <- as.integer(crashes_tracts_geo$tot_crashes)
E <- as.integer(crashes_tracts_geo$population)
K <- ncol(design_matrix)
xs <- design_matrix
I <- length(unique(crashes_tracts_geo$tract_id))
i_index <- crashes_tracts_geo$tract_id
J <- length(unique(crashes_tracts_geo$time_id))
j_index <- crashes_tracts_geo$time_id
N_edges <- ncol(nbs_adj)
neighbors <- nbs_adj
tau <- get_scaling_factor(nyc_nbs_connected)

data_full_1 <- list(
  N = N,
  y = y,
  E = E,
  K = K,
  xs = xs,
  I = I,
  J = J,
  i_index = i_index,
  j_index = j_index,
  N_edges = N_edges,
  neighbors = neighbors,
  tau = tau
)

full_model_1 <- cmdstan_model(full_model_1_file)
set.seed(50)

full_fit_1 <- full_model_1$sample(data=data_full_1, 
                                  parallel_chains=cores, 
                                  refresh=0)

full_summary <- full_fit_1$summary()
vars <- c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'betas[5]', 'sigma', 'rho', 'nb_disp')
full_summary_subset <- full_summary[full_summary$variable %in% vars, ]
numeric_cols <- sapply(full_summary_subset, is.numeric)
full_summary_subset[numeric_cols] <- round(full_summary_subset[numeric_cols], 3)

print(full_summary_subset)
```

```{r class.source=NULL, message=FALSE, warning=FALSE}
# Posterior Checks
y_rep_full <- as_draws_matrix(full_fit_1$draws("y_rep"))
ppc_central_interval(y_rep_full, data_full_1$y)
```

```{r class.source=NULL, message=FALSE, warning=FALSE}
ppc_y_yrep_overlay(y_rep_full, data_full_1$y,
                     'Poisson model PPC\ny (blue dot) vs. y_rep (yellow 50% central interval, grey full extent)')
```

# Diagnostic Steps

## Visualize residuals by space and time


